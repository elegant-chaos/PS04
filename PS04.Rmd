---
title: "STAT/MATH 495: Problem Set 04"
author: "Jenn Halbleib"
date: "2017-10-03"
output:
  html_document:
    toc: true
    toc_float: true
    toc_depth: 2
    collapsed: false
    smooth_scroll: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.width=8, fig.height=4.5, message=FALSE)
set.seed(76)
```

# Collaboration

Please indicate who you collaborated with on this assignment:


# Load packages, data, model formulas

```{r, warning=FALSE}
library(tidyverse)
credit <- read_csv("http://www-bcf.usc.edu/~gareth/ISL/Credit.csv") %>%
  select(-X1) %>%
  mutate(ID = 1:n()) %>% 
  select(ID, Balance, Income, Limit, Rating, Age, Cards, Education)
```

You will train the following 7 models on `credit_train`...

```{r}
model1_formula <- as.formula("Balance ~ 1")
model2_formula <- as.formula("Balance ~ Income")
model3_formula <- as.formula("Balance ~ Income + Limit")
model4_formula <- as.formula("Balance ~ Income + Limit + Rating")
model5_formula <- as.formula("Balance ~ Income + Limit + Rating + Age")
model6_formula <- as.formula("Balance ~ Income + Limit + Rating + Age + Cards")
model7_formula <- as.formula("Balance ~ Income + Limit + Rating + Age + Cards + Education")
```

... where `credit_train` is defined below, along with `credit_test`.

```{r}
set.seed(79)
credit_train <- credit %>% 
  sample_n(20)
credit_test <- credit %>% 
  anti_join(credit_train, by="ID")
```


# RMSE vs number of coefficients

```{r, echo=TRUE, warning=FALSE, message=FALSE}
# Placeholder vectors of length 7, filled with zeros
RMSE_train <- rep(0,7)
RMSE_test <- rep(0,7)


# Do your work here:

#Model1 
model1.1 <- lm(model1_formula, data = credit_train)
predictionsTrain1.1 <- predict(model1.1, credit_train)
predictionsTest1.1 <- predict(model1.1, credit_test)
RMSE_test[1] <- sqrt(sum((predictionsTest1.1-credit_test$Balance)^2)/length(credit_test))
RMSE_train[1] <- sqrt(sum((predictionsTrain1.1-credit_train$Balance)^2)/length(credit_train))

#Model2 
model1.2 <- lm(model2_formula, data = credit_train)
predictionsTrain1.2 <- predict(model1.2, credit_train)
predictionsTest1.2 <- predict(model1.2, credit_test)
RMSE_test[2] <- sqrt(sum((predictionsTest1.2-credit_test$Balance)^2)/length(credit_test))
RMSE_train[2] <- sqrt(sum((predictionsTrain1.2-credit_train$Balance)^2)/length(credit_train))

#Model3 
model1.3 <- lm(model3_formula, data = credit_train)
predictionsTrain1.3 <- predict(model1.3, credit_train)
predictionsTest1.3 <- predict(model1.3, credit_test)
RMSE_test[3] <- sqrt(sum((predictionsTest1.3-credit_test$Balance)^2)/length(credit_test))
RMSE_train[3] <- sqrt(sum((predictionsTrain1.3-credit_train$Balance)^2)/length(credit_train))

#Model4 
model1.4 <- lm(model4_formula, data = credit_train)
predictionsTrain1.4 <- predict(model1.4, credit_train)
predictionsTest1.4 <- predict(model1.4, credit_test)
RMSE_test[4] <- sqrt(sum((predictionsTest1.4-credit_test$Balance)^2)/length(credit_test))
RMSE_train[4] <- sqrt(sum((predictionsTrain1.4-credit_train$Balance)^2)/length(credit_train))

#Model5
model1.5 <- lm(model5_formula, data = credit_train)
predictionsTrain1.5 <- predict(model1.5, credit_train)
predictionsTest1.5 <- predict(model1.5, credit_test)
RMSE_test[5] <- sqrt(sum((predictionsTest1.5-credit_test$Balance)^2)/length(credit_test))
RMSE_train[5] <- sqrt(sum((predictionsTrain1.5-credit_train$Balance)^2)/length(credit_train))

#Model6 
model1.6 <- lm(model6_formula, data = credit_train)
predictionsTrain1.6 <- predict(model1.6, credit_train)
predictionsTest1.6 <- predict(model1.6, credit_test)
RMSE_test[6] <- sqrt(sum((predictionsTest1.6-credit_test$Balance)^2)/length(credit_test))
RMSE_train[6] <- sqrt(sum((predictionsTrain1.6-credit_train$Balance)^2)/length(credit_train))

#Model7 
model1.7 <- lm(model7_formula, data = credit_train)
predictionsTrain1.7 <- predict(model1.7, credit_train)
predictionsTest1.7 <- predict(model1.7, credit_test)
RMSE_test[7] <- sqrt(sum((predictionsTest1.7-credit_test$Balance)^2)/length(credit_test))
RMSE_train[7] <- sqrt(sum((predictionsTrain1.7-credit_train$Balance)^2)/length(credit_train))

# Save results in a data frame. Note this data frame is in wide format.
results <- data_frame(
  num_coefficients = 1:7,
  RMSE_train,
  RMSE_test
) 

# Some cleaning of results
results <- results %>% 
  # More intuitive names:
  rename(
    `Training data` = RMSE_train,
    `Test data` = RMSE_test
  ) %>% 
  # Convert results data frame to "tidy" data format i.e. long format, so that we
  # can ggplot it
  gather(type, RMSE, -num_coefficients)

ggplot(results, aes(x=num_coefficients, y=RMSE, col=type)) +
  geom_line() + 
  labs(title = "RMSE with a small training set", x="# of coefficients", y="RMSE", col="Data used to evaluate \nperformance of fitted model")
```


# Interpret the graph

Compare and contrast the two curves and hypothesize as to the root cause of any differences.

In this case, the testing data set curve shows a larger RMSE than the training data set curve. With a training set of only 20 cases, once an adequate number of predictor variables has been added to the model (here, 3 predictors appears to capture a reasonable amount of the variance in $y_{observed}$), the training data curve should reflect an RMSE near 0. The $RMSE_{train}$ limits to 0 because a data set of size $n=20$ with a strong linear relationship is (relatively) easily explained by linear model. As $n$ grows larger in the case of the testing data set, more nonlinear and outlier points increase the $RMSE_{test}$. Further, using a training data set of size $n=20$ reduces the variability the model is "exposed" to before make predictions, a factor that also increases $RMSE_{test}$. 

Regarding the number of variables chosen, the RMSE of the test set reflects normal expectations: when too many of the variables available are included in the model, overfitting to the training data and to the potentially multicollinear variables leads to an increase in $RMSE_{test}$. 

# Bonus

Repeat the whole process, but let `credit_train` be a random sample of size 380
from `credit` instead of 20. Now compare and contrast this graph with the
one above and hypothesize as to the root cause of any differences.

Here, I have recreated the models 1-7 with a training data set of size $n=380$ and a testing data set of size $n=20$.
```{r}
set.seed(79)
credit_train2 <- credit %>% 
  sample_n(380)
credit_test2 <- credit %>% 
  anti_join(credit_train2, by="ID")
```

```{r, echo=TRUE, warning=FALSE, message=FALSE}
# Placeholder vectors of length 7, filled with zeros
RMSE_train2 <- rep(0,7)
RMSE_test2 <- rep(0,7)

#Model1 
model2.1 <- lm(model1_formula, data = credit_train2)
predictionsTrain2.1 <- predict(model2.1, credit_train2)
predictionsTest2.1 <- predict(model2.1, credit_test2)
RMSE_test2[1] <- sqrt(sum((predictionsTest2.1-credit_test2$Balance)^2)/length(credit_test2))
RMSE_train2[1] <- sqrt(sum((predictionsTrain2.1-credit_train2$Balance)^2)/length(credit_train2))

#Model2 
model2.2 <- lm(model2_formula, data = credit_train2)
predictionsTrain2.2 <- predict(model2.2, credit_train2)
predictionsTest2.2 <- predict(model2.2, credit_test2)
RMSE_test2[2] <- sqrt(sum((predictionsTest2.2-credit_test2$Balance)^2)/length(credit_test2))
RMSE_train2[2] <- sqrt(sum((predictionsTrain2.2-credit_train2$Balance)^2)/length(credit_train2))

#Model3 
model2.3 <- lm(model3_formula, data = credit_train2)
predictionsTrain2.3 <- predict(model2.3, credit_train2)
predictionsTest2.3 <- predict(model2.3, credit_test2)
RMSE_test2[3] <- sqrt(sum((predictionsTest2.3-credit_test2$Balance)^2)/length(credit_test2))
RMSE_train2[3] <- sqrt(sum((predictionsTrain2.3-credit_train2$Balance)^2)/length(credit_train2))

#Model4 
model2.4 <- lm(model4_formula, data = credit_train2)
predictionsTrain2.4 <- predict(model2.4, credit_train2)
predictionsTest2.4 <- predict(model2.4, credit_test2)
RMSE_test2[4] <- sqrt(sum((predictionsTest2.4-credit_test2$Balance)^2)/length(credit_test2))
RMSE_train2[4] <- sqrt(sum((predictionsTrain2.4-credit_train2$Balance)^2)/length(credit_train2))

#Model5
model2.5 <- lm(model5_formula, data = credit_train2)
predictionsTrain2.5 <- predict(model2.5, credit_train2)
predictionsTest2.5 <- predict(model2.5, credit_test2)
RMSE_test2[5] <- sqrt(sum((predictionsTest2.5-credit_test2$Balance)^2)/length(credit_test2))
RMSE_train2[5] <- sqrt(sum((predictionsTrain2.5-credit_train2$Balance)^2)/length(credit_train2))

#Model6 
model2.6 <- lm(model6_formula, data = credit_train2)
predictionsTrain2.6 <- predict(model2.6, credit_train2)
predictionsTest2.6 <- predict(model2.6, credit_test2)
RMSE_test2[6] <- sqrt(sum((predictionsTest2.6-credit_test2$Balance)^2)/length(credit_test2))
RMSE_train2[6] <- sqrt(sum((predictionsTrain2.6-credit_train2$Balance)^2)/length(credit_train2))

#Model7 
model2.7 <- lm(model7_formula, data = credit_train2)
predictionsTrain2.7 <- predict(model2.7, credit_train2)
predictionsTest2.7 <- predict(model2.7, credit_test2)
RMSE_test2[7] <- sqrt(sum((predictionsTest2.7-credit_test2$Balance)^2)/length(credit_test2))
RMSE_train2[7] <- sqrt(sum((predictionsTrain2.7-credit_train2$Balance)^2)/length(credit_train2))

# Save results in a data frame. Note this data frame is in wide format.
results2 <- data_frame(
  num_coefficients = 1:7,
  RMSE_train2,
  RMSE_test2
) 

# Some cleaning of results
results2 <- results2 %>% 
  # More intuitive names:
  rename(
    `Training data` = RMSE_train2,
    `Test data` = RMSE_test2
  ) %>% 
  # Convert results data frame to "tidy" data format i.e. long format, so that we
  # can ggplot it
  gather(type, RMSE, -num_coefficients)

ggplot(results2, aes(x=num_coefficients, y=RMSE, col=type)) +
  geom_line() + 
  labs(title = "RMSE with a large training set", x="# of coefficients", y="RMSE", col="Data used to evaluate \nperformance of fitted model")
```

#RMSE vs. Training Set Size

The following code creates a side-by-side line plot of the RMSE for each model 1-7 for both the small and large training set.

```{r}
allResults <- results %>% full_join(results2, by = c("num_coefficients","type")) %>% rename(SmallTrainSet = RMSE.x, LargeTrainSet = RMSE.y)

allResults <- allResults %>% gather(DataSet, RMSE, ends_with("TrainSet"))

ggplot(allResults, aes(x=num_coefficients, y=RMSE, col=type)) +
  geom_line() + 
  labs(title = "Comparing RMSE of a small and a large training set", x="# of coefficients", y="RMSE", col="Data used to evaluate \nperformance of fitted model") + facet_grid(.~DataSet)
```

In comparing these graphs, it is difficult to discern whether our new model, fitted with a training set of size $n=380$, improves on the model fitted with a training set of size $n=20$. Since the test set size of the new model is relatively small ($n=20$), we expect the $RMSE_{test}$ to trend towards 0 (since the training set has a high probability of containing points that teach the model about the overall sample variability). 

The largest difference between the 2 models is the behavior of $RMSE_{test}$ when we increase the number of predictors in the model. With a small test set, more predictors led to an increase in $RMSE_{test}$. However, with a large test set, the model is "exposed" to more of the variability caused by adding more predictors, so the $RMSE_{test}$ did not show an increase related to added predictors.